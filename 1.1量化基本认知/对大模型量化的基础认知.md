#  大模型量化的基本认知

##  大模型量化的本质

​	将**浮点型**的参数和输入值，转为**整型**。从而降低模型推理的**內存占用**，提高**推理速度**。也能降低模型的**存储大小**



​	$WX=  (\frac{W}{scale} * scale)X = W \prime X * scale $​



​	将**浮点矩阵乘**，转为了**整数矩阵乘**，和**浮点点乘**





## 大模型量化所要经历的步骤

- **量化**:将**模型参数**和**输入值**从浮点数范围，通过**缩放因子**和**零点**，转到整数范围。

- **反量化**:将模型推理得到的结果，从整数范围，转换回浮点数范围。





## 大模型量化所面临的问题

让经历**量化**和**反量化**等到的推理结果，与原来的推理结果**尽可能接近**。



**示例**（以对称量化为例）：选取**绝对值最大的数**，以其绝对值与127的比作为缩放因子

​	**（1）一个浮点数矩阵**

​	$$ W= \begin{bmatrix} 1.21 & -1.31 & 0.22 \\\\0.83 & 2.11 & -1.53  \\\\
​                     0.79 & -0.54 & 0.84 \end{bmatrix} $$,

​	

​	**（2）量化：映射到int8（-127，127的范围）**:

​	$scale(缩放因子) = \frac{2.11}{127}$

​	$$ A = W \div scale \approx \begin{bmatrix} 73 & -79 & 13 \\\\ 50 & 127 & -92 \\\\ 48 & -33 & 51 
\end{bmatrix} $$

​	

​	**（3）反量化：映射回浮点数范围**

​	$W \prime = A * scale = \begin{bmatrix}  1.21 & -1.31 & 0.22 \\\\  0.83 & 2.11 & -1.53 \\\\ \textcolor{red}{0.80} & \textcolor{red}{-0.55} & \textcolor{red}{0.85}  \end{bmatrix} $​​  

​	**经历量化和反量化后，因为精度问题，得到结果与原来结果有误差**





# 各种量化方法（int8为例子）

## 对称量化

$scale = \frac{max(|x|)}{255}$​ 

$W\prime =  \frac{W}{scale}$​

$AW = (scaleA*A\prime)(scaleW*W\prime) = A\prime W\prime * scaleA * scaleW$



## 非对称量化

$scale = \frac{max-min}{255}$

$zeroPoint =frac{|min|}{scale}$

$W\prime = \frac{W}{scale}+zeroPoint$​

$AW = (A\prime-zeroPointA)*scaleA @(W\prime-zeroPointW)*scaleW$

$=(A\prime W\prime-zeroPointA*W\prime-zeroPointW*A\prime+zeroPointA*zeroPointW)*scaleA*scaleW$



​	

# 大模型量化的方式

## 训练后动态量化

​	**解释**:在输入进入每一层之前，对层的**权重参数**和**输入**进行**量化**，对**输出结果**进行反**量化**（每次激活都要**动态计算量化参数**）

​	**缺点**: 每层推理都要对激活值计算量化参数，耗时；要用浮点型存激活值，耗空间，耗时



## 训练后静态量化

​	**解释**:用代表性数据（训练数据中选）跑一遍模型，估算每一层的激活值量化参数；对每一层的输出，该层反量化后，直接用下一次激活值量化参数进行量化，再存入显存。



## 量化感知训练

​	在**模型训练**时，引入对**激活值**和**模型参数**的**量化**和**反量化**，让模型**适用量化环境**



## LLM.int()（HuggingFace Transform 库中量化方法）

  **提出背景**:随着模型参数的增加，**传统量化方法**下的模型**精度急剧下降**。

  **精度急剧下降的原因**:

（1）模型复杂度，推理层数增加，误差被逐渐扩大

（2）参数的总误差变大

（3）**某些维度的特征突然变得很大，导致量化误差变得很大**

​	（0.01,0.22,255.00） 量化反量化后变成(0,0,1000000)

  **原理**:提取特征比较突出的维度对应的行，与权重矩阵中对应的列，拆分成两个矩阵分别进行乘法，最后相加。

​	（对非突出特征值的运算采用量化，突出特征值的运算采用浮点运算）

**特点**:节省空间，因为权重参数是量化后存储的。但比一般推理耗时，因为计算很复杂。但精度较高。



## QLoRa

**量化的本质**:相对连续的浮点数空间，映射到相对离散的整数空间。

**背景**:大模型中权重参数的分布，可通过某些方法，使其近似正态分布。

**思路**:将数值分布更加密集的区域，映射到更多的整数。

**具体做法**:等累计概率密度地分配映射整数。

**优点**:相对传统的量化，保真性更高。

**缺点**:因为是非线性映射，所以量化后得到的参数不能直接参与运算，必须反量化后才能参与运算。

​	（显存优化方案，无法提高运算速度）



## GPTQ

**思路**:对某个权重矩阵量化时，依次量化权重向量，然后调整尚未量化的权重向量，“校准”以减少误差

**原理**:非常复杂，设计大量数学推理，看不懂



## ‌**SmoothQuant**









## ‌**LLM-QAT**‌











​	



# 一些辅助大模型研究的平台

huggingface









​	



​	

